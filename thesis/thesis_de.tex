\documentclass[twoside,a4paper,fleqn,12pt]{article}
\usepackage{fancyhdr,a4wide,graphicx}
\usepackage[paper=a4paper,left=20mm,right=20mm,top=25mm,bottom=25mm]{geometry}
\pagestyle{fancy}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[]{amsfonts}
\usepackage{amsmath}
\usepackage{color}
\usepackage{colortbl}
%\usepackage{mathptmx}
\usepackage[bitstream-charter]{mathdesign}
\usepackage{charter}
\usepackage{helvet}
\usepackage{courier}
\usepackage{verbatim}
\usepackage{sectsty}
\usepackage{listings}
\usepackage{scalefnt}
\usepackage{setspace}
\usepackage{ngerman}

\definecolor{darkred}{rgb}{.5,0,0}
\definecolor{darkblue}{rgb}{0,0,.5}
\usepackage[plainpages=false,pdfpagelabels,colorlinks=true,urlcolor=darkblue,pagecolor=darkred,citecolor=darkred,linkcolor=darkred]{hyperref}
%\newcommand\url[1]{\texttt{#1}}

\lstset{basicstyle=\ttfamily\small,lineskip=-0.5em}

% define the title
\author{Frank Richter 68278\\frank.richter@gmail.com}
\title{\usefont{OT1}{phv}{b}{n}\selectfont Entwicklung eines Compilers für eine auf Cg basierende Sprache zur Programmierung von Graphikkarten \normalfont}
\date{\today}

\begin{document}

\newcommand\btxandlong{und}
\newcommand\btxandshort{u}
\newcommand\Btxinlong{In}
\newcommand\Btxinshort{I}
\newcommand\btxpageslong{Seiten}
\newcommand\btxetalshort{et al}
\newcommand\btxeditionlong{Auflage}
\bibliographystyle{mystyle}

% Zeilenabstand 1.5
\renewcommand{\baselinestretch}{1.50}\normalsize

% Helvetica für Section-Titel
\allsectionsfont{\usefont{OT1}{phv}{b}{n}\selectfont}

% Different font in captions
\newcommand{\captionstyle}{\small\centering}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionstyle #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionstyle #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   % Cancel the effect of \makeatletter

% Fussnoten: alle Zeilen einrücken
\makeatletter
\newlength{\myFootnoteWidth}
\newlength{\myFootnoteLabel}
\setlength{\myFootnoteLabel}{1.2em}%  <-- can be changed to any valid value
\renewcommand{\@makefntext}[1]{%
  \setlength{\myFootnoteWidth}{\columnwidth}%
  \addtolength{\myFootnoteWidth}{-\myFootnoteLabel}%
  \noindent\makebox[\myFootnoteLabel][r]{\@makefnmark\ }%
  \parbox[t]{\myFootnoteWidth}{#1}%
}
\makeatother

% ---------- normal title ---------- %
\titlepage
\maketitle
\thispagestyle{empty}
\newpage
\thispagestyle{empty}
\mbox{}

% ---------- Fancyheader ---------- %
\fancyhead[L]{}
\fancyhead[R]{}
\fancyfoot[C]{\today}
\fancyfoot[R]{\footnotesize \thepage{}}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{24pt}

% generates the title


% ---------- table of contents ---------- %
\newpage
\pagenumbering{roman}
%\addcontentsline{toc}{section}{Inhaltsverzeichnis}
\pdfbookmark[1]{Inhaltsverzeichnis}{myPDFtocLabel}
\tableofcontents

\cleardoublepage
\pagenumbering{arabic}
\newcommand\todo[1]{\footnote{\textcolor{red}{TODO: #1}}}
\newcommand\fcite[1]{\footnote{\cite{#1}}}
\newcommand\fciteX[2]{\footnote{\cite{#1}, #2}}

\section{Zielstellung}

In der Echtzeit-3D-Graphik werden für 3D-Objekte überwiegend Dreiecksnetze\footnote{engl. ``triangle mesh'' und kürzer ``mesh''; siehe z.B. \cite{watt_de}}
verwendet. Dies spiegelt sich in dem Aufbau von 3D-Graphikprozessoren~("`GPU"') wie auch
den Programmierschnittstellen~(\cite{glspec4}, \cite{dx10}) wieder. Insbesondere bei der Programmierung der GPU müssen
separate Vertex- und Fragmentprogramme % Ref oder Erklärung was Vertex/Fragment
erstellt werden. Diese Aufteilung sowie die Definition der "`Schnittstelle"' zwischen den Verarbeitungseinheiten muss vom Programmierer manuell vorgenommen werden.

Ziel dieser Arbeit ist es, einen Compiler zu entwickeln, der die Aufteilung in Vertex- und Fragmentprogramme (und auch Schnittstellendefinition)
automatisch vornimmt,
ohne dass der Programmierer explizit angeben muss, auf welcher der Funktionseinheiten ein bestimmter Befehl ausgeführt wird.
Die zu compilierende Sprache wird in Abschnitt~\ref{langspec} spezifiziert. Die Compilerimplementierung wird in Abschnitt~\ref{implementation}
beschrieben.

% Nochmal Abschnitt mit kurzer Beschreibung wichtiger Konzepte? (meshes, Vertices vs Fragmente, ...)

\section{Sprachspezifikation}
\label{langspec}

\input{langspec}

\section{Implementierung}
\label{implementation}

\subsection{Compiler-Aufbau}
\begin{figure}[h]
   \centering
  \includegraphics{compiler_structure}
  \caption{Schematischer Aufbau des Compilers}
  \label{fig:structure}
\end{figure}

Der Aufbau entspricht grösstenteils dem, was für einen Compiler üblich ist: das \emph{Front-End} generiert nach Syntax- und Semantikanalyse
eine Repräsentation des Programms in einem \emph{Zwischencode}. Auf dieser Zwischenrepräsentation werden im "`Middle-End"' % Ref wo das gesagt wird, oder besseres Wort
Optimierungen vorgenommen. Im letzten Schritt wird im \emph{Back-End} aus der optimierten Zwischenrepräsentation der tatsächliche Zielcode generiert.

Besonderheit dieses Compilers ist der Schritt \emph{Auftrennung VP/FP}. Hier wird für jeden Befehl der Zwischenrepräsentation untersucht, mit
welcher Berechnungsfrequenz~(siehe \ref{Berechnungsfrequenz})\footnote{Es wird auch "`Meshfrequenz"' definiert, aber der Einfachheit halber
trennt diese Implementierung bloß in Vertex- und Fragmentfrequenz. Das Konzept ist jedoch auch auf weitere Frequenzen erweiterbar.}
jeder Befehl des Programms ausgeführt werden muss - mit anderen Worten,
es wird untersucht, welche Befehle auf der Vertex-Einheit oder der Fragment-Einheit ausgeführt werden müssen. Mit diesen Informationen kann
das Programm entsprechend in ein Vertex- und ein Fragment-Programm aufgeteilt werden. Da zur Laufzeit auch ein "`Übergeben"' von Ausgaben
des Vertexprogramms an Eingaben des Fragmentprogramms stattfindet wird auch eine den Vertex-Ausgaben zu Fragment-Eingaben
abbildende "`Schnittstelle"' generiert.

Die Programme werden vom Aufspalter in Zwischencode ausgegeben und können noch einmal optimiert werden. % Irgendein besonderer Vorteil?
Abschließend werden ein Fragment- und ein Vertexprogramm im gewünschten Zielcode ausgegeben\footnote{Diese Implementierung benutzt den
gleichen Generator für beide Programme, prinzipiell könnten diese jedoch mit verschiedenen Generatoren ausgegeben werden.}.

\subsection{Implementierungsdetails}

Als \emph{Programmiersprache}, in der die hier beschriebene Implementierung verfasst ist, wurde C++ gewählt.
Gründe dafür sind:
\begin{itemize}
\item Die Flexibilität der Sprache und deren reichhaltige Standardbibliothek,
\item hohe Portabilität (C++-Compiler sind für praktisch jede Plattform verfügbar),
\item eine reichhaltige Palette an von Dritten hergestellter Bibliotheken,
\item die einfache Bindbarkeit an andere Sprachen (direkt oder über eine C-kompatible Schnittstelle),
\item nicht zuletzt die Gewandheit des Autors dieser Arbeit in C++.
\end{itemize}
 
Um eine Wiederverwendung des Compilers zu vereinfachen wurde dieser im Wesentlichen als eine \emph{Bibliothek} realisiert;
eine "`Kommandozeilenversion"' des Compilers setzt auch auf diese Bibliothek auf.
 
Zur Sicherstellung der fortwährend korrekten Funktionsweise aller Module des Compilers wurden entwicklungsbegleitend 
jeweils \emph{Tests} der Module geschrieben (Black-Box und White-Box); Ausführen der Tests war regelmässiger Teil des Entwicklungsprozesses.
 
\subsection{Lexer}

Der \emph{Lexer} wandelt die als Byte-Strom vorliegende Eingabe in eine Folge von "`Tokens"'.
Ein Token ist eine der in Abschn.~\ref{Lexikalische Einheiten} aufgezählten lexikalischen Einheiten, ein bekanntes Symbol (Operatoren etc.) oder Schlüsselwort. 
Leerzeichen (``Whitespace'') und Kommentare werden bereits vom Lexer ignoriert (d.h. für diese werden keine Tokens produziert).

Der Lexer folgt einer typischen Implementierung wie sie z.B. in \cite{wirth_compiler} beschrieben ist. Auf einige beachtenswerte Aspekte
wird im folgenden eingegangen.

\paragraph{Eingabe.} Da die Spezifikation von Unicode als Eingabe ausgeht arbeitet, der Lexer entsprechend auf der Basis von Unicode-kodierten Zeichen.
Der Byte-Strom der Eingabe wird also in einem Schritt noch vor dem Lexer in einen "`Unicode-Strom"' umkodiert\footnote{Verwendet wird dazu die Bibliothek ICU,
\url{http://site.icu-project.org/}}.

\paragraph{Schlüsselwörter.} Erkennt der Lexer einen Bezeichner, wird auch geprüft, ob es sich um ein Schlüsselwort handelt. Ist dies der Fall
wird im Token eine dem Schlüsselwort eindeutig zugeordnete ID gespeichert.

Die erste Ausnahme allerdings bilden die Schlüsselwörter für Vektor- und Matrixtypen (Abschn.~\ref{Vektortypen}, \ref{Matrixtypen}). Diese entsprechen
jeweils dem Muster $\mathit{typ}\mathrm{N}$ bzw. $\mathit{typ}\mathrm{N}\mathit{x}\mathrm{M}$ (mit $N \in 1 \dots 4, M \in 1 \dots 4$).
Da eine eigene ID für jeden Vektor- oder Matrix insgesamt 20 weitere IDs pro Basis-Typ nach sich ziehen würde -- wobei später noch jeder ID wiederum
die ursprünglichen Werte für $N$ und $M$ nochmals zugeordnet
werden müssten -- wird generierten Token vermerkt, ob es sich um einen Vektor- oder Matrixtyp handelt.
Dazu überprüft der Lexer, ob ein Bezeichner den angegebenen Muster für Vektor- bzw. Matrixschlüsselwörtern entspricht.
Weiterhin werden bereits $N$ bzw. $M$ aus den Bezeichnern extrahiert und ebenfalls vermerkt.

\begin{figure}[h]
   \centering
  \lstinputlisting[language=C++]{snips/LexerToken.txt}
  \caption{Daten eines vom Lexer ausgegebenen Tokens}
  \label{fig:LexerToken}
\end{figure}

Die zweite Ausnahme bilden Attributnamen~(\ref{Attribute}) inklusive Swizzles~(\ref{Vektorattribute}). Diese sind teilweise recht allgemein, und
es erscheint wünschenswert, Bezeichner zuzulassen, die Attributnamen entsprechen -- z.B. ``length'', das auch ein Arrayattribut ist, und einbuchstabige
Bezeichner wie ``x'', ``y'' etc., welche auch Vektorattribute (Swizzles) sind. Syntaktisch gibt es keine Mehrdeutigkeiten zwischen Attributen und
anderen Bezeichnern -- ein Attribut kann \emph{nur} rechts eines \op{.} auftauchen, ein anderer Bezeichner dort nie.

Weiterhin schreibt die Spezifikation vor, dass zwei Bezeicher als identisch betrachtet werden, wenn sie kanonisch äquivalent im Sinne von Unicode sind.
Zu diesem Zweck werden alle Bezeichner vom Lexer nach einer von Unicode vorgegebenen Form normalisiert~(\cite{unicode}, Annex \#15).

\subsection{Parser}

Der \emph{Parser} (oder auch "`Scanner"') untersucht den vom Lexer gelieferten Strom von Tokens auf syntaktische Strukturen:
einerseits wird überprüft, ob der Token-Strom einem Wort der in der Sprachspezifikation gegebenen Sprache entspricht.
Andererseits werden wären der Überprüfung bereits syntaktische Elemente extrahiert um diese bei der semantischen Verarbeitung zu
verwenden.

Beispiel: Soll aus dem Token-Strom ein "`Ausdruck"'~(\ref{ausdruck}) gelesen werden, prüft der Parser, ob die Tokens der Regel eines gültigen
Ausdrucks folgen -- ist dies nicht der Fall, handelt es sich um einen \emph{Syntaxfehler}.
Gleichzeitig werden aber auch Informationen wie numerische Werte oder Bezeichner gespeichert, da diese für eine später folgende
\emph{semantische} Verarbeitung notwendig sind.

%- Lookahead: unendl.
%- Grammatik: kontextfrei, rechtsrekursiv

\subsubsection{Eigenschaften der Grammatik}

Die Grammatik ist kontextfrei. Die Regeln sind rechtsrekursiv.

\subsubsection{Aufbau des Parsers}
%- SemanticHandler

Der Parser ist handprogrammiert nach der Methode des rekursiven Abstiegs. % Ref Wirth?

Die semantische Verarbeitung wird an ein Interface vom Typ \verb+SemanticHandler+ übergeben.
Dieses Interface übernimmt die verschiedenen Aspekte der semantischen Verarbeitung, von der Verwaltung der
Symboltabellen bis zu einer geeigneten internen Repräsentation von Ausdrücken.

\verb+SemanticHandler+ besitzt Methoden, um syntaktische Elemente - wie Bezeichner und numerische Literale - in Objekte 
einzukapseln. Diese Objekte wiederum werden in der semantischen Verarbeitung anderer syntaktischer Elemente
zurück an das Interface übergeben.

Beispiel: Bei einem Ausdruck \verb+a * 2+ werden von \verb+SemanticHandler+ zunächst Repräsentationen für
\verb+a+ und \verb+2+ erfragt. Zurückgegeben werden Repräsentationen von "`Ausdrücken"'. Diese wiederum
dienen als Argumente, um eine Repräsentation einer Multiplikation zu erhalten. Diese letzte Repräsentation kann
dann überall dort verwendet werden, wo Ausdrücke erwartet werden: Zuweisungen, Funktionsparameter etc.

Vom \verb+SemanticHandler+ zum Parser gibt es auch einen "`Rückkanal"'. Dies ist nötig, da bei einigen
Konstrukten bekannt sein muss, ob ein Bezeichner eine Variable, eine Funktion oder einen Typ identifiziert.
(Beispiel: \verb+foo (1)+ - ist \verb+foo+ eine Funktion, so ist dies ein Funktionsaufruf; ist es ein Typ-Alias, so ist es
der Aufruf eines Typ-Konstruktors; ist \verb+foo+ eine Variable, so ist dies ein ungültiger Ausdruck.)

Die Fehlerbehandlung bei der syntaktischen wie auch semantischen Verarbeitung wird über \emph{Ausnahmen}
realisiert. Der Parser selbst fängt dabei Ausnahmen ab, um zu gewährleisten, dass möglichst viel eines
Programms verarbeitet wird, um möglichst viele potentielle Fehler aufzudecken: % Wirth
tritt z.B. eine Ausnahme während der Verarbeitung eines Block-Kommandos auf, setzt der Parser die
Verarbeitung nach dem nächsten Semikolon -- also mit dem nächsten Kommando -- fort (sofern kein Ende
des Blockes festgestellt wird).
Die abgefangen Ausnahmen werden jedoch nicht verworfen, sondern an ein Objekt zur Fehlerbehandlung
übergeben (um z.B. eine Ausgabe auf der Konsole vorzunehmen).

Die Idee hinter dem \verb+SemanticHandler+ ist eine möglichst vollständige Trennung zwischen syntaktischer
und semantischer Verarbeitung. Eine minimale Implementierung könnte intern eine (annähernde)
AST-Repräsentation generieren (nur "`annähernd"' da der Parser einige semantische Informationen über
Bezeichner benötigt). In der tatsächlichen Implementierung wird vom \verb+SemanticHandler+ allerdings 
gleich eine Umsetzung in die Zwischenrepräsentation vorgenommen (Beschreibung siehe Unten):
die Generierung eines ASTs wurde als Zwischenschritt wurde als unnötigen angesehen, da die Zwischenrepräsentation
wichtige semantische Eigenschaften (wie Typinformationen) erhält und sich die Verarbeitungsschritte "`Auftrennung"'
und "`Optimierung"' auf dieser besser vornehmen lassen.

\subsection{Zwischenrepräsentation}

- SSA

\subsection{Generator}



\section{Ausblick}

\cleardoublepage
\appendix
\bibliography{thesis_de}
\addcontentsline{toc}{section}{Literatur}

\end{document}
